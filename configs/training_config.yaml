# Training configuration for crisis-agent fine-tuning
# Optimized for: 16GB VRAM, 4 CPU cores, 32GB RAM
# Adjust these parameters based on your GPU memory and dataset size

training:
  # Output directories
  output_dir: "outputs/checkpoints"
  logging_dir: "outputs/logs"
  final_model_name: "final"  # Name for the final checkpoint directory (e.g., "final", "crisis_agent_v1", "model_2026-01-25")
  
  # Training parameters (quality-preserving optimizations)
  num_epochs: 3  # Keep original for quality
  per_device_train_batch_size: 2  # Keep original for stability
  per_device_eval_batch_size: 2  # Keep original
  gradient_accumulation_steps: 4  # Keep original
  learning_rate: 2.0e-4
  warmup_steps: 100  # Keep original for learning stability
  max_steps: -1  # -1 means use num_epochs instead
  
  # Optimization
  optim: "adamw_torch"
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Learning rate schedule
  lr_scheduler_type: "cosine"
  
  # Checkpointing (safe speed optimization - doesn't affect quality)
  save_steps: 1000  # Reduced frequency (was 500) - only affects I/O overhead
  save_total_limit: 2  # Keep fewer checkpoints (was 3) - only affects disk space
  save_strategy: "steps"
  
  # Evaluation (safe speed optimization - doesn't affect training quality)
  eval_steps: 1000  # Reduced frequency (was 500) - only affects evaluation overhead
  eval_strategy: "steps"  # Keep evaluation enabled for monitoring
  
  # Logging (safe speed optimization - doesn't affect quality)
  logging_steps: 25  # Less frequent logging (was 10) - only reduces I/O overhead
  report_to: []  # Set to ["wandb"] if using Weights & Biases
  
  # Other
  fp16: true
  bf16: false
  dataloader_num_workers: 4  # Matches 4 CPU cores (increase if you have more cores)
  remove_unused_columns: true  # Set to true to remove unused columns (faster)
  seed: 42
  
  # Memory optimization (for 16GB VRAM)
  # If you encounter OOM errors, try:
  # - Reduce per_device_train_batch_size to 1
  # - Increase gradient_accumulation_steps to 8
  # - Reduce max_seq_length in model_config.yaml to 1024

# Optional: export to GGUF after training (LM Studio, Ollama)
# Uses the in-memory model so no extra load step. Set to false to skip and run scripts/export_gguf.py later.
export_gguf:
  enabled: true
  output_dir: "outputs/gguf"   # Directory for GGUF files (relative to project root)
  quantization: "q4_k_m"        # q4_k_m (default), q8_0, f16; see scripts/export_gguf.py --list-quantizations
  # Push to Hugging Face Hub after export (optional). Leave empty to skip.
  push_to_hub: ""              # e.g. "username/my_model_gguf"
  push_to_hub_private: false
